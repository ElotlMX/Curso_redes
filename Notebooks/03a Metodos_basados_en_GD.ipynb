{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de optimizción basados en gradiente\n",
    "\n",
    "El gradiente descendiente es el método estándar para realizar optimización de los pesos en una red neuronal. Pero existen diferentes formas de implementar esta optimización, como por ejemplo: Sotchastic Gradient Descent (SGD), Batch Gradient Descent o Mini-batch Gradient Descent. \n",
    "\n",
    "Una parte importante del método de gradiente descendiente es la elección de rango de aprendizaje. Existen métodos que buscan estimar un rango de aprendizaje que pueda permitir una convergencia de los pesos de la red adecuada. Aquí exploramos dos de estos métodos: 1) Adagrad y 2) Adam. Además comparamos con un grdiente descendiente stocástico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de entrenamiento\n",
    "\n",
    "Elegimos una tarea de clasificiación binaria simple y preparamos el conjunto de datos supervisados que utilizaremos para los distintos métodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>¿es animal?</th>\n",
       "      <th>¿es mamífero?</th>\n",
       "      <th>¿es felino?</th>\n",
       "      <th>¿es doméstico?</th>\n",
       "      <th>¿tiene dos orejas?</th>\n",
       "      <th>¿es negro?</th>\n",
       "      <th>¿tiene cuatro patas?</th>\n",
       "      <th>¿es gato?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ¿es animal?   ¿es mamífero?   ¿es felino?   ¿es doméstico?  \\\n",
       "0             1               1             1                1   \n",
       "1             0               0             0                1   \n",
       "2             1               0             1                1   \n",
       "3             1               1             0                1   \n",
       "4             1               1             1                0   \n",
       "5             1               1             1                1   \n",
       "6             1               0             0                1   \n",
       "7             1               1             1                1   \n",
       "8             1               0             0                1   \n",
       "9             0               0             0                0   \n",
       "10            1               1             1                0   \n",
       "11            1               1             1                0   \n",
       "12            1               1             1                1   \n",
       "13            1               1             1                1   \n",
       "\n",
       "     ¿tiene dos orejas?   ¿es negro?   ¿tiene cuatro patas?   ¿es gato?  \n",
       "0                     1            1                      1           1  \n",
       "1                     0            1                      0           0  \n",
       "2                     0            1                      1           0  \n",
       "3                     1            0                      1           0  \n",
       "4                     1            0                      1           0  \n",
       "5                     0            0                      0           1  \n",
       "6                     1            1                      0           0  \n",
       "7                     0            0                      1           1  \n",
       "8                     0            0                      0           0  \n",
       "9                     0            0                      0           0  \n",
       "10                    1            1                      1           0  \n",
       "11                    1            0                      1           0  \n",
       "12                    1            0                      1           1  \n",
       "13                    1            0                      0           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Abrir los datos\n",
    "data = pd.read_csv('cat_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de vectores de entrenamiento: 14, con dimensión: 7\n"
     ]
    }
   ],
   "source": [
    "#Convertir los datos a numpy\n",
    "npData = data.to_numpy()\n",
    "#Ejemplos\n",
    "X = npData[:,:-1]\n",
    "#Clases de los ejemplos\n",
    "Y = npData[:,-1]\n",
    "\n",
    "#Tamaño de los datos\n",
    "#Unidades de entrada\n",
    "N,d = X.shape\n",
    "\n",
    "print('Número de vectores de entrenamiento: {}, con dimensión: {}'.format(N,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos los datos en dos dimensiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reducción de dimensionalidad\n",
    "X_plot = PCA(2).fit_transform(X)\n",
    "#Visualización\n",
    "plt.scatter(X_plot[:,0],X_plot[:,0], c=Y)\n",
    "plt.title('Ploteo de datos de entrenamiento')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez preparados los datos de entrenamiento, procedemos a estimar los pesos de la red; para esto, utilizamos en primer lugrar SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos basados en GD\n",
    "\n",
    "Para explorar los diferentes métodos que se basan en el concepto del gradiente descendiente, entrenaremos una red neuronal a la que aplicaremos una capa oculta. En todos los casos, la arquitectura de la red es como sigue:\n",
    "\n",
    "* Una capa oculta con tres unidades y activación $\\tanh$.\n",
    "* Una capa de salida con dos unidades (para cada clase) con activación Softmax.\n",
    "* La función de riesgo estará determinada por la entropía cruzada: $$R(\\theta) = \\sum_x \\sum_y y \\ln \\phi_y(x)$$ Donde $\\phi_y(x)$ es la salida de la red en la clase dada ($y=0$ ó $y=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "En el Stochastic Gradient Descent o SGD se tiene que:\n",
    "\n",
    "* Se actualizan los pesos cada vez que se observa un ejemplo. \n",
    "* El rango de aprendizaje es un hiperparámetro fijo, esto es, no varía durante el entrenamiento.\n",
    "\n",
    "La actualización de los pesos se hace como:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} R(\\theta;x,y)$$\n",
    "\n",
    "En este caso, el gradiente $\\nabla_{\\theta} R(\\theta;x,y)$ está evaluado sobre sólo un par de entrenamiento $(x,y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en iteración 25: 5\n",
      "Error en iteración 50: 6\n",
      "Error en iteración 75: 4\n",
      "Error en iteración 100: 4\n",
      "Error en iteración 125: 6\n",
      "Error en iteración 150: 2\n",
      "Error en iteración 175: 2\n",
      "Error en iteración 200: 2\n",
      "Error en iteración 225: 2\n",
      "Error en iteración 250: 2\n",
      "Error en iteración 275: 2\n",
      "Error en iteración 300: 2\n",
      "Error en iteración 325: 2\n",
      "Error en iteración 350: 2\n",
      "Error en iteración 375: 2\n",
      "Error en iteración 400: 2\n",
      "Error en iteración 425: 2\n",
      "Error en iteración 450: 2\n",
      "Error en iteración 475: 2\n",
      "Error en iteración 500: 2\n",
      "Error en iteración 525: 2\n",
      "Error en iteración 550: 2\n",
      "Error en iteración 575: 2\n",
      "Error en iteración 600: 2\n",
      "Error en iteración 625: 2\n",
      "Error en iteración 650: 2\n",
      "Error en iteración 675: 2\n",
      "Error en iteración 700: 2\n",
      "Error en iteración 725: 2\n",
      "Error en iteración 750: 2\n",
      "Error en iteración 775: 2\n",
      "Error en iteración 800: 2\n",
      "Error en iteración 825: 2\n",
      "Error en iteración 850: 2\n",
      "Error en iteración 875: 2\n",
      "Error en iteración 900: 2\n",
      "Error en iteración 925: 2\n",
      "Error en iteración 950: 2\n",
      "Error en iteración 975: 2\n",
      "Error en iteración 1000: 2\n",
      "CPU times: user 3 s, sys: 153 ms, total: 3.15 s\n",
      "Wall time: 2.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(0)\n",
    "\n",
    "#HIPERPARÁMETROS\n",
    "its = 1000\n",
    "eta = 1\n",
    "\n",
    "#Inicializar pesos\n",
    "h_dim = 3\n",
    "W1 = np.random.rand(h_dim,d)/np.sqrt(d)\n",
    "b1 = np.ones(h_dim)\n",
    "W2 = np.random.rand(2,h_dim)/np.sqrt(h_dim)\n",
    "b2 = np.ones(2)\n",
    "\n",
    "t = 0\n",
    "stop = False\n",
    "while stop == False:\n",
    "    error = 0\n",
    "    for x,y in zip(X,Y):\n",
    "        #FORWARD\n",
    "        #Capa oculta\n",
    "        a1 = np.dot(W1,x)+b1\n",
    "        h = np.tanh(a1)\n",
    "        #Capa de salida\n",
    "        a2 = np.dot(W2,h)+b2\n",
    "        exp = np.exp(a2)\n",
    "        f = exp/exp.sum(0)\n",
    "        \n",
    "        #BACKWARD\n",
    "        #Backpropagation capa de salida\n",
    "        d_out = exp/exp.sum(0)\n",
    "        d_out[y] -= 1\n",
    "        #Backpropagation capa oculta\n",
    "        d_h = (1-h**2)*np.dot(W2.T,d_out)        \n",
    "\n",
    "        #Derivadas\n",
    "        DW2 = np.outer(d_out,h)\n",
    "        Db2 = d_out\n",
    "        DW1 = np.outer(d_h,x)\n",
    "        Db1 = d_h\n",
    "        \n",
    "        #Actualización de pesos\n",
    "        #El rango de aprendizaje es fijo\n",
    "        W2 -= eta*DW2\n",
    "        b2 -= eta*Db2\n",
    "        W1 -= eta*DW1\n",
    "        b1 -= eta*Db1\n",
    "        \n",
    "        #Error cuadrático\n",
    "        error += (np.argmax(f)-y)**2\n",
    "        \n",
    "    t += 1\n",
    "    #Imprime el error cada 25 its.\n",
    "    if t % 25 == 0:\n",
    "        print('Error en iteración {}: {}'.format(t,error))\n",
    "    #Condición de finalización\n",
    "    if error == 0 or t == its:\n",
    "        stop = True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos definir una función forward para observar el resultado de la red con los pesos aprendidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 1 0 0 0 0 1 1] [1 0 0 0 0 1 0 1 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "def forward(X):\n",
    "    a1 = np.dot(X,W1.T)+b1\n",
    "    h = np.tanh(a1)\n",
    "    a2 = np.dot(h,W2.T)+b2\n",
    "    exp = np.exp(a2)\n",
    "    f = exp/exp.sum(1, keepdims=True)\n",
    "    \n",
    "    return f\n",
    "\n",
    "print(np.argmax(forward(X), axis=1),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAGRAD\n",
    "\n",
    "El método de Adagrad se basa en el SGD, pero aquí el rango de aprendizaje varía, y las actualizaciones se realizan según la regla:\n",
    "\n",
    "$$\\theta_i \\leftarrow \\theta_i - \\frac{\\eta}{\\sqrt{\\mu}+\\epsilon} \\nabla_{\\theta_i} R(\\theta)$$\n",
    "\n",
    "Donde $\\mu$ es un parámetro que varía según la siguiente regla:\n",
    "\n",
    "$$\\mu \\leftarrow \\mu + [\\nabla_{\\theta_i} R(\\theta)]^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en iteración 1: 7\n",
      "Error en iteración 2: 6\n",
      "Error en iteración 3: 6\n",
      "Error en iteración 4: 6\n",
      "Error en iteración 5: 6\n",
      "Error en iteración 6: 6\n",
      "Error en iteración 7: 6\n",
      "Error en iteración 8: 6\n",
      "Error en iteración 9: 6\n",
      "Error en iteración 10: 5\n",
      "Error en iteración 11: 5\n",
      "Error en iteración 12: 5\n",
      "Error en iteración 13: 5\n",
      "Error en iteración 14: 4\n",
      "Error en iteración 15: 5\n",
      "Error en iteración 16: 5\n",
      "Error en iteración 17: 5\n",
      "Error en iteración 18: 3\n",
      "Error en iteración 19: 2\n",
      "Error en iteración 20: 2\n",
      "Error en iteración 21: 2\n",
      "Error en iteración 22: 3\n",
      "Error en iteración 23: 3\n",
      "Error en iteración 24: 3\n",
      "Error en iteración 25: 3\n",
      "Error en iteración 26: 2\n",
      "Error en iteración 27: 2\n",
      "Error en iteración 28: 2\n",
      "Error en iteración 29: 0\n",
      "CPU times: user 168 ms, sys: 29.1 ms, total: 197 ms\n",
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(0)\n",
    "\n",
    "##HIPERPARÁMETROS\n",
    "its = 1000\n",
    "#Rango de aprendizahe inicial\n",
    "eta = 1\n",
    "#Epsilon\n",
    "eps = 1e-8\n",
    "\n",
    "#Inicializar pesos\n",
    "h_dim = 3\n",
    "W1 = np.random.rand(h_dim,d)/np.sqrt(d)\n",
    "b1 = np.ones(h_dim)\n",
    "W2 = np.random.rand(2,h_dim)/np.sqrt(h_dim)\n",
    "b2 = np.ones(2)\n",
    "\n",
    "#Inicialización del parámetro mu\n",
    "#Se utiliza uno para cada matriz de pesos\n",
    "mu1 = 0\n",
    "mub1 = 0\n",
    "mu2 = 0\n",
    "mub2 = 0\n",
    "\n",
    "t = 0\n",
    "stop = False\n",
    "while stop == False:\n",
    "    error = 0\n",
    "    for x,y in zip(X,Y):\n",
    "        #FORWARD\n",
    "        #Capa oculta\n",
    "        a1 = np.dot(W1,x)+b1\n",
    "        h = np.tanh(a1)\n",
    "        #Capa de salida\n",
    "        a2 = np.dot(W2,h)+b2\n",
    "        exp = np.exp(a2)\n",
    "        f = exp/exp.sum(0)\n",
    "        \n",
    "        #BACKWARD\n",
    "        #Backpropagation capa de salida\n",
    "        d_out = exp/exp.sum(0)\n",
    "        d_out[y] -= 1\n",
    "        #Backpropagation capa oculta\n",
    "        d_h = (1-h**2)*np.dot(W2.T,d_out)        \n",
    "\n",
    "        #Derivadas\n",
    "        DW2 = np.outer(d_out,h)\n",
    "        Db2 = d_out\n",
    "        DW1 = np.outer(d_h,x)\n",
    "        Db1 = d_h\n",
    "        \n",
    "        #ADAGRAD\n",
    "        #Actualizació de mu\n",
    "        mu1 += DW1**2\n",
    "        mub1 += Db1**2\n",
    "        mu2 += DW2**2\n",
    "        mub2 += Db2**2\n",
    "        \n",
    "        #Cada matriz de pesos se actualiza por Adagrad\n",
    "        W2 -= (eta/(np.sqrt(mu2)+eps))*DW2\n",
    "        b2 -= (eta/(np.sqrt(mub2)+eps))*Db2\n",
    "        W1 -= (eta/(np.sqrt(mu1)+eps))*DW1\n",
    "        b1 -= (eta/(np.sqrt(mub1)+eps))*Db1\n",
    "        \n",
    "        #Error cuadrático\n",
    "        error += (np.argmax(f)-y)**2\n",
    "        \n",
    "    t += 1\n",
    "    print('Error en iteración {}: {}'.format(t,error))\n",
    "    #Condición de paro\n",
    "    if error == 0 or t == its:\n",
    "        stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM\n",
    "\n",
    "Es un método basado en SGD, pero donde el rango de aprendizaje y los valores de cambio varían, siendo que la actualización de los pesos de la red se actualizan por medio de la regla:\n",
    "\n",
    "$$\\theta_i \\leftarrow \\theta_i - \\frac{\\eta}{\\sqrt{\\hat{\\nu}} + \\epsilon} \\hat{m}$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$$\\hat{m} = \\frac{m}{1-\\beta_1}$$\n",
    "\n",
    "y:\n",
    "\n",
    "$$\\hat{\\nu} = \\frac{\\nu}{1-\\beta_2}$$\n",
    "\n",
    "Tal que $m$ es un parámetro que se actualiza como:\n",
    "\n",
    "$$m \\leftarrow \\beta_1 m + (1-\\beta_1) \\nabla_\\theta R(\\theta)$$\n",
    "\n",
    "Mientras que $\\nu$ es actualizado como:\n",
    "\n",
    "$$\\nu \\leftarrow \\beta_2 \\nu + (1-\\beta_2) [\\nabla_\\theta R(\\theta)]^2$$\n",
    "\n",
    "Aquí, $\\beta_1, \\beta_2\\in [0,1]$ son dos hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en iteración 1: 7\n",
      "Error en iteración 2: 7\n",
      "Error en iteración 3: 6\n",
      "Error en iteración 4: 6\n",
      "Error en iteración 5: 5\n",
      "Error en iteración 6: 4\n",
      "Error en iteración 7: 5\n",
      "Error en iteración 8: 4\n",
      "Error en iteración 9: 3\n",
      "Error en iteración 10: 3\n",
      "Error en iteración 11: 3\n",
      "Error en iteración 12: 3\n",
      "Error en iteración 13: 3\n",
      "Error en iteración 14: 0\n",
      "CPU times: user 150 ms, sys: 19.3 ms, total: 169 ms\n",
      "Wall time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(0)\n",
    "\n",
    "##HIPERPARÁMETROS\n",
    "#Núm de iteraciones\n",
    "its = 1000\n",
    "#Para ADAM\n",
    "#Rango de aprendizaje inicial\n",
    "eta = 1\n",
    "#Epsilón\n",
    "eps = 1e-8\n",
    "#hiperparámetros beta1 y beta2\n",
    "beta1 = 0.0009\n",
    "beta2 = 0.0009\n",
    "\n",
    "#Inicializar pesos\n",
    "h_dim = 3\n",
    "W1 = np.random.rand(h_dim,d)/np.sqrt(d)\n",
    "b1 = np.ones(h_dim)\n",
    "W2 = np.random.rand(2,h_dim)/np.sqrt(h_dim)\n",
    "b2 = np.ones(2)\n",
    "\n",
    "#Inicialización de m y nu\n",
    "#Se tiene uno por cada matriz de pesos\n",
    "m1 = 0\n",
    "mb1 = 0\n",
    "m2 = 0\n",
    "mb2 = 0\n",
    "v1 = 0\n",
    "vb1 = 0\n",
    "v2 = 0\n",
    "vb2 = 0\n",
    "\n",
    "t = 0\n",
    "stop = False\n",
    "while stop == False:\n",
    "    error = 0\n",
    "    for x,y in zip(X,Y):\n",
    "        #FORWARD\n",
    "        #Capa oculta\n",
    "        a1 = np.dot(W1,x)+b1\n",
    "        h = np.tanh(a1)\n",
    "        #Capa de salida\n",
    "        a2 = np.dot(W2,h)+b2\n",
    "        exp = np.exp(a2)\n",
    "        f = exp/exp.sum(0)\n",
    "        \n",
    "        #BACKWARD\n",
    "        #Backpropagation capa de salida\n",
    "        d_out = exp/exp.sum(0)\n",
    "        d_out[y] -= 1\n",
    "        #Backpropagation capa oculta\n",
    "        d_h = (1-h**2)*np.dot(W2.T,d_out)        \n",
    "\n",
    "        #Derivadas\n",
    "        DW2 = np.outer(d_out,h)\n",
    "        Db2 = d_out\n",
    "        DW1 = np.outer(d_h,x)\n",
    "        Db1 = d_h\n",
    "        \n",
    "        #ADAM\n",
    "        #Actualización Primer momento\n",
    "        m1 = beta1*m1 + (1-beta1)*DW1\n",
    "        mb1 = beta1*mb1 + (1-beta1)*Db1\n",
    "        m2 = beta1*m2 + (1-beta1)*DW2\n",
    "        mb2 = beta1*mb2 + (1-beta1)*Db2\n",
    "        #Actualización Segundo momento\n",
    "        v1 = beta2*v1 + (1-beta2)*DW1**2\n",
    "        vb1 = beta2*vb1 + (1-beta2)*Db1**2\n",
    "        v2 = beta2*v2 + (1-beta2)*DW2**2\n",
    "        vb2 = beta2*vb2 + (1-beta2)*Db2**2\n",
    "        \n",
    "        #Ponderación m\n",
    "        m1_p = m1/(1-beta1)\n",
    "        mb1_p = mb1/(1-beta1)\n",
    "        m2_p = m2/(1-beta1)\n",
    "        mb2_p = mb2/(1-beta1)\n",
    "        #Ponderación nu\n",
    "        v1_p = v1/(1-beta2)\n",
    "        vb1_p = vb1/(1-beta2)\n",
    "        v2_p = v2/(1-beta2)\n",
    "        vb2_p = vb2/(1-beta2)\n",
    "        \n",
    "        #Actualización de pesos con ADAM\n",
    "        W2 -= (eta/(np.sqrt(v2_p)+eps))*m2_p\n",
    "        b2 -= (eta/(np.sqrt(vb2_p)+eps))*mb2_p\n",
    "        W1 -= (eta/(np.sqrt(v1_p)+eps))*m1_p\n",
    "        b1 -= (eta/(np.sqrt(vb1_p)+eps))*mb1_p\n",
    "        \n",
    "        #Error cuadrático\n",
    "        error += (np.argmax(f)-y)**2\n",
    "        \n",
    "    t += 1\n",
    "    print('Error en iteración {}: {}'.format(t,error))\n",
    "    #Condición de paro\n",
    "    if error == 0 or t == its:\n",
    "        stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede observarse, el algoritmo de aprendizaje no converge correctamente en el método de SGD, pues debemos ajustar adecuadamente el rango de aprendizaje, lo que puede ser un proceso costoso.\n",
    "\n",
    "En los otros métodos, Adagrad y Adam, la convergencia se da adecuadamente, el rango de aprendizaje se adapta al problema en ambos casos. A pesar de esto, la convergencia se da de diferente manera; en general, para este problema específico, el método de Adam parece una mejor opción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
